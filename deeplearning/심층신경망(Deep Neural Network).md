*인공신경망과 연결되는 내용임

# 심층신경망
0. 심층신경망 과정
1. 손실함수
2. 신경망 최적화 
    - 경사하강법(+역전파)
5. 드롭아웃

### 0. 심층 신경망 과정
1. 정방향으로 신경망의 네트워크로 전파될 때, 현 단계 뉴런의 <u>**가중치**</u>와 전 단계 뉴련의 출력값 곱들의 합을 입력값으로 받는다. 
2. 이 값은 다시 <b>활성화 함수</b>를 통해 다음 뉴런으로 전파된다.
3. 최종적으로 출력층에서 나오는 값은 이 모델이 예측한 결괏값이다.
4. 심층신경망을 출력을 기반으로 <b>손실함수</b>를 이용하여 <b>오차</b>를 계산한다.
5. 심층신경망 네트워크에 있는 모든 <u>**가중치**</u>에 대한 도함수를 찾아서 오차를 역전파한다.
6. 5를 통해 모든 <u>**가중치**</u>와 편향을 업데이트 한다. 
- 반복적인 순전파와 역전파릍 통해 최적의 가중치들과 편향값들을 찾았다면 모델 학습이 잘된 것이다.
- 해당 출력을 모델의 예측값으로 사용할 수 있다.

### 1. 손실함수
- 회귀 모델 손실함수
    - MSE(평균제곱오차)
        - 특이치에 민감하다.(큰 오차에 민감하다)
        - 제곱된 단위이기에 해석이 직관적이지 않다 -> rmse 사용
        - 미분 가능하여 모델 학습에 유리
        - <img width="218" height="76" alt="image" src="https://github.com/user-attachments/assets/e5ad5f83-f15d-4378-a52d-3aea8bbbe122" />

    - MAE(평균절대오차)
      - 이상치(특이치)에 강인하다
      - 해석이 직관적이다
      - 미분 불가능하여 최적화가 까다롭거나 불안정
      - <img width="203" height="72" alt="image" src="https://github.com/user-attachments/assets/796b5c13-773d-4eb3-bc82-05f9b246bc76" />

- 분류 모델 손실함수
    - 크로스 엔트로피(Cross Entropy)
        - 실제값과 예측값의 차이가 클수록 더 큰 손실값 부여되도록 디자인된 함수
        - 이진분류
            - 이진 교차 엔트로피 오차(BCEE)
            - Positive일 때 ($y_i=1$): 수식은 $-\log(\hat{y}_i)$만 남습니다. 예측 확률 $\hat{y}_i$가 1에 가까울수록 손실은 0에 가까워지고, 0에 가까울수록 손실은 무한대에 가까워집니다.
            - Negative일 때 ($y_i=0$): 수식은 $-\log(1 - \hat{y}_i)$만 남습니다. 예측 확률 $\hat{y}_i$가 0에 가까울수록 손실은 0에 가까워집니다.
            - <img width="427" height="67" alt="image" src="https://github.com/user-attachments/assets/1cc482ac-75f7-40a0-82ac-9e1e053bb175" />

        - 다중분류
            - 범주형 교차 엔트로피 오차(CCEE)
            - $y_{i,k}$: $i$번째 데이터가 $k$번째 클래스에 속하는지 여부 (실제 정답, One-Hot Encoding 형태로 0 또는 1)
            - $\hat{y}_{i,k}$: $i$번째 데이터가 $k$번째 클래스일 예측 확률 (소프트맥스 출력)
            - <img width="293" height="67" alt="image" src="https://github.com/user-attachments/assets/625d825f-3ea1-402b-9844-b073144397b5" />
    - <img width="479" height="176" alt="image" src="https://github.com/user-attachments/assets/efaed25f-2c23-4e2e-88d9-5ab99e2ef4ee" />


### 2. 신경망 최적화 
- 순전파(Forward Pass): 입력 데이터가 신경망을 통과하여 최종 예측값과 **손실(오차)**을 계산합니다.
- 역전파(Backward Pass): 계산된 손실을 줄이기 위해, 역전파 알고리즘을 사용하여 모든 가중치에 대한 기울기를 계산합니다.
- 경사 하강법(Update): 역전파로 계산된 기울기를 경사 하강법(또는 Adam 등 옵티마이저) 수식에 넣어 가중치를 실제 업데이트합니다.
### 경사하강법(Batch GD: Update)
  - <u>신경망 최적화</u> 기법 중 대표적이다. 신경망 최적화란 오차를 최소화하는 파라미터(w, b)를 찾는 과정
  - 손실함수는 가중치 에 대한 함수이다. 
  - 파라미터 업데이트시 손실함수의 기울기를 사용하여 <br/> 손실함수의 경사가 작아지는 “방향”으로 파라미터를 조절한다.
  - 손실함수가 볼록함수 형태라면 **”미분”**으로 손실이 가장 작은 가중치를 찾을 수 있지만 <br/> 딥러닝의 손실함수는 복잡하고 어려워 **”경사하강법”**을 사용한다. 
    - 미분 
        - 손실 함수의 최저점을 찾는 방향을 제시한다
        - 미분값이 0이 되는 지점을 단 한 번의 대수적 계산으로 바로 찾을 수 있음 (전역 최솟값)
        - 미분은 "여기서 아래로 내려가려면 어느 방향으로 얼마나 가야 하는가?"라는 질문에 대한 **방향(기울기)**을 계산해 줍니다.
    - 경사 하강법은 "미분이 알려준 방향으로 조금씩 발걸음을 옮겨가며 최저점에 도달하는 반복적인 행위"입니다.
    - <img width="259" height="129" alt="image" src="https://github.com/user-attachments/assets/3359acd3-47cf-4f67-b69c-9405448d6883" />
  - 단점
      - 정확하게 가중치를 찾지만 파라미터 한 번 업데이트 할 때 데이터셋 전체를 사용함 -> 속도 느림, 조기종료 가능
  - 단점 해결책
      - 확률적 경사하강법(Stochastic GD, SGD)
          - 파라미터 업데이트 위해 무작위로 샘플링된 1개 데이터 사용
          - 적은 계산량, 적절한 기울기 / 노이즈 심함
      - 미니배치 경사하강법(Mini-Batch GD)
          - 랜덤 추출한 일부 데이터를 통해 가중치 조절
          - Ex.학습데이터1000개, 배치크기100, 총10개 미니배치 -> 미니배치 하나당 한번씩 경사하강법 진행
  - 경사하강법 최적화 기법
      - Momentum
      - Adagrad
      - RMSprop
      - Adam
  - <img width="584" height="318" alt="image" src="https://github.com/user-attachments/assets/4e332e0e-dd18-43c2-a84f-b4d2e138d21a" /> 
  *이미지 출처: https://mmmsk.ai.kr/Study/Deep-Learning/%eb%94%a5%eb%9f%ac%eb%8b%9d-%ea%b8%b0%eb%b3%b8/6-1.-%ec%b5%9c%ec%a0%81%ed%99%94-%ec%95%8c%ea%b3%a0%eb%a6%ac%ec%a6%98

### 3. 드롭아웃
- 신경망 훈련 동안 반복마다 은닉 유닛의 일부를 확률 p만큼 랜덤하게 드롭해서 학습에 참여하지 않도록 하는 방법
- 과대적합 방지하고 일반화 성능을 높인다
- 순전파에서 은닉 노드가 드롭되므로 역전파에서도 제외됨

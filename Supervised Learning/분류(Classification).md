# 분류
1. 로지스틱 회귀
2. 의사결정나무
3. 앙상블
  - 보팅
  - 배깅
  - 부스팅
4. 랜덤 포레스트
5. 그래디언트 부스팅

### 1. 로지스틱 회귀
    - 시그모이드 함수 사용하여 확률 변환 -> 임계값 설정하여 분류 
    - From sklearn.linear_model import LogisticRegression
        - 하이퍼파라미터
            - max_iter : 알고리즘의 수렴을 위한 반복 최대 횟수 지정
            - Penalty : 규제 종류 지정(l1, l2, elasticnet, none)
            - C : 규제의 강도 조절. 값 클수록 규제 약해짐
        - 학습 후 갖는 속성
            - coef_ : 학습된 모형 특성의 가중치 추정값
            - intercept_ : 학습된 모델의 절편 추정값

### 2. 의사결정나무
    - Feature를 조건 기반으로 참 거짓으로 나눠 스무고개 하듯 학습 이어가기 
    - 장점 
        - 결과의 해석이 가능
        - 선형성없음. 수학적 가정 불필요
        - 트리기반은 범주/연속형 수치 모두 예측 가능
    - 단점
        - 과대적합 확률 높음
        - 트리 구조로 선형성 떨어짐
        - 출력변수가 연속형인 회귀모델에서는 예측력 떨어질 수 있음
    - From sklearn.tree import DecisionTreeClassifier
        - 하이퍼파라미터
            - max_depth : 깊어질 수 있는 최대 깊이. (default=None)과대적합 방지용
            - max_features : 최대로 사용할 feature 개수. 과대적합 방지용
            - min_samples_split : 트리의 노드가 가지고 있는 최소한의 샘플 수. (Default=2)과대적합 방지용
        - 결과 시각화 내용
            - 분할 조건
                - 데이터를 왼쪽 가지와 오른쪽 가지로 나누는 기준.
                - 이 기준은 가장 불순도(Impurity)를 크게 낮추는 방향으로 설정
            - 지니 불순도
                - 해당 노드에 포함된 데이터들이 얼마나 균일하지 않은지를 나타내는 척도
                - 분류 문제에서 나무를 분할하는 주된 기준
                - 값의 범위: 0~0.5(값 높을수록 불순도 높음)
            - 샘플 수 
                - 현재 해당 노드에 도달한 전체 학습 데이터의 개수
            - 클래스 분포
                - 해당 노드에 포함된 샘플들이 각 클래스별로 몇 개씩 존재하는지
            - 최종 예측 클래스

### 3. 앙상블
    - 다수의 기본 모델을 생성하고 결합하여 하나의 새로운 모델을 생성하는 것
    - 단일 모델에 비해 성능 우수
    - 편향과 분산 고려하기에 과적합 방지 용이
    - 고려할 것
        - 어떤 모델 사용?
        - 어떻게 결합?
    - 대표적 앙상블 모델
        - 랜덤포레스트
        - 그래디언트 부스팅
    - 앙상블 기법 
      - 1. 보팅 
      - 2.배깅 
        - 랜덤 포레스트 : 의사결정나무 병렬 결합 
      - 3.부스팅 
        - 그래디언트 부스팅 : 의사결정나무 순차 결합
        
  #### 1. 보팅
    - 여러 모델의 결과를 기반으로 투표에 의해 결과 도출
    - 하드 보팅
        - 각 모델의 결과 중 가장 많이 분류(다수결)된 결과로 최종 결과 선정
    - 소프트 보팅
        - 각 모델별 예측한 확률값의 평균으로 최종값 선정
        - 각 모델은 예측 확률을 출력함
        
  #### 2. 배깅
    - <u>부트스트랩 기반 샘플링 기법</u>을 통해 하나의 알고리즘을 학습하여 생성된 여러 모델의 결과를 결합하는 알고리즘
    - 부트스트랩은 원본 데이터에서 <b>샘플을 여러 번 복원 추출</b>하는 과정을 반복하는 샘플링 방법
    - 병렬 학습
    - 순서
        - 부트스트랩 샘플 데이터 생성
        - 각 부트스트랩 데이터로 다수의 개별 모델 학습
        - 최종 예측을 위한 <u>보팅</u> 진행
        
  #### 3. 부스팅
    - 예측력 약한 여러 모델 순차적으로 연결하여 예측력 강한 모델 만들기 
    - 모델을 직렬 결합한다
    - 앞선 모델의 틀린 예측에 가중치를 부여 하여 틀린 데이터를 더 잘 맞추도록 학습함
    - 순서
        - 학습 데이터의 관측치를 동일한 가중치로 세팅하여 학습 진행 , 모델 예측
        - 오분류된 관측치에 높은 가중치 부여, 다시 샘플링과 학습 진행
        - 각 모델의 예측 결과를 결합할 때 각 모델에 가중치를 주어 가중 평균 계산, 최종값 출력
    - 대표 모델
        - Adaboost
        - Gradient boosting
        - XGBoost
        - LightGBM

### 4. 랜덤포레스트 
    - 의사결정나무를 다수로 학습시켜 결과 종합하는 배깅 기반 앙상블 알고리즘 
    - 다수의 나무를 구성함으로써 과대적합 방지 가능
    - From sklearn.ensemble import RandomForestClassifier
        - 하이퍼파라미터
            - max_depth : 깊어질 수 있는 최대 깊이. 과대적합 방지용(default = None)
            - n_estimators : 앙상블하는 트리 개수 (default=100)
            - max_features : 최대 사용할 feature 개수. 과대적합 방지용
            - min_samples_split : 트리가 분할할 때 최소 샘플 개수.과대적합 방지용. (default=2) 
            
### 5. 그래디언트 부스팅
    - 트리 기반 모델 직렬 연결, 틀린 데이터에 가중치 부여 하는 부스팅 계열 알고리즘
    - 장점
        - 어려운 데이터에 성능 좋을 수 있음
        - 트리기반 특성상 특성의 스케일 조정하지 않아도됨
        - 이진 분류나 연속적 수치 예측 작동 좋음
    - 단점
        - 트리기반 특성상 고차원 데이터 작동 좋지 않음
        - 훈련시간 오래 걸림
    - From sklearn.ensemble import GradientBoostingClassifier
        - 하이퍼파라미터
            - learning_rate : 학습률. 너무 크면 성능 저하, 너무 작으면 학습 속도 느림. n_estimators와 같이 튜닝. default=0.1
            - n_estimators : 부스팅 스테이지 수 default=100. 커질수록 과대적합 가능
            - max_depth : 트리 깊이, 과대적합 방지용. Default=3
            - Subsample : 샘플 사용 비율, 과대적합 방지용. default=1.0
            - max_features : 최대 사용할 feature 비율, 과대적합 방지용. Default =1.0
            
## 사용 예시 
```python 
From sklearn.ensemble import GradientBoostingClassifier

gbc = GradientBoostingClassifier(random_state=0)
gbc.fit(x_train, y_train)

acc_train_gbc = gbc.score(x_train, y_train)
acc_test_gbc = gbc.score(x_test, y_test)
```
